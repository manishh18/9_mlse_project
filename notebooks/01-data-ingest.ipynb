{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2df25511",
   "metadata": {},
   "source": [
    "# 01 - Data ingest: Wikipedia + Wikimedia Commons\n",
    "Goal: download raw page text and up to N images per monument, save metadata (license & captions).\n",
    "Seed monuments: Taj Mahal, Qutub Minar, Red Fort, Delhi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a314dabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directories. Monuments: ['Taj Mahal', 'Qutub Minar', 'Red Fort, Delhi']\n",
      "RAW_TEXT: /Users/jaydobariya/Desktop/RAG Project/data/raw/text\n",
      "RAW_IMAGES: /Users/jaydobariya/Desktop/RAG Project/data/raw/images\n",
      "META: /Users/jaydobariya/Desktop/RAG Project/data/meta\n",
      "PROCESSED: /Users/jaydobariya/Desktop/RAG Project/data/processed\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 — folders and monument list\n",
    "from pathlib import Path\n",
    "import os, json\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "PARENT = ROOT.parent\n",
    "\n",
    "RAW_TEXT = PARENT / \"data\" / \"raw\" / \"text\"\n",
    "RAW_IMAGES = PARENT / \"data\" / \"raw\" / \"images\"\n",
    "META = PARENT / \"data\" / \"meta\"\n",
    "PROCESSED = PARENT / \"data\" / \"processed\"\n",
    "\n",
    "for p in [RAW_TEXT, RAW_IMAGES, META, PROCESSED]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Seed monuments - edit later to add more\n",
    "MONUMENTS = [\n",
    "    \"Taj Mahal\",\n",
    "    \"Qutub Minar\",\n",
    "    \"Red Fort, Delhi\"\n",
    "]\n",
    "\n",
    "print(\"Created directories. Monuments:\", MONUMENTS)\n",
    "print(\"RAW_TEXT:\", RAW_TEXT)\n",
    "print(\"RAW_IMAGES:\", RAW_IMAGES)\n",
    "print(\"META:\", META)\n",
    "print(\"PROCESSED:\", PROCESSED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2ba44b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 — helpers\n",
    "import requests, time\n",
    "from urllib.parse import unquote\n",
    "from tqdm import tqdm\n",
    "\n",
    "USER_AGENT = \"rag-monuments-bot/0.1 (your-email@example.com)\"\n",
    "session = requests.Session()\n",
    "session.headers.update({\"User-Agent\": USER_AGENT})\n",
    "\n",
    "WIKI_API = \"https://en.wikipedia.org/w/api.php\"\n",
    "COMMONS_API = \"https://commons.wikimedia.org/w/api.php\"\n",
    "SLEEP = 0.6  # polite sleep\n",
    "\n",
    "def sanitize_filename(s):\n",
    "    return \"\".join(c if (c.isalnum() or c in \"._- \") else \"_\" for c in s).strip()\n",
    "\n",
    "def safe_get(url, params=None, timeout=20):\n",
    "    r = session.get(url, params=params, timeout=timeout)\n",
    "    time.sleep(SLEEP)\n",
    "    r.raise_for_status()\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9927cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 — fetch page text\n",
    "def fetch_wikipedia_text(title):\n",
    "    params = {\"action\":\"query\",\"prop\":\"extracts\",\"explaintext\":1,\"titles\":title,\"format\":\"json\",\"redirects\":1}\n",
    "    r = safe_get(WIKI_API, params=params)\n",
    "    j = r.json()\n",
    "    pages = j.get(\"query\", {}).get(\"pages\", {})\n",
    "    for pid, pg in pages.items():\n",
    "        return pg.get(\"title\"), pg.get(\"extract\", \"\")\n",
    "    return title, \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe4abf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 — fetch image file titles from a wiki page\n",
    "def fetch_image_file_titles(title):\n",
    "    params = {\"action\":\"query\",\"prop\":\"images\",\"titles\":title,\"format\":\"json\",\"imlimit\":\"max\",\"redirects\":1}\n",
    "    r = safe_get(WIKI_API, params=params)\n",
    "    j = r.json()\n",
    "    files = []\n",
    "    pages = j.get(\"query\", {}).get(\"pages\", {})\n",
    "    for pid, pg in pages.items():\n",
    "        for im in pg.get(\"images\", []) or []:\n",
    "            if im.get(\"title\", \"\").lower().startswith(\"file:\"):\n",
    "                files.append(im[\"title\"])\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b5e979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 — get commons image info (url, extmetadata)\n",
    "def get_commons_image_info(file_title):\n",
    "    params = {\"action\":\"query\",\"titles\":file_title,\"prop\":\"imageinfo\",\"iiprop\":\"url|extmetadata|size|mime\",\"format\":\"json\"}\n",
    "    r = safe_get(COMMONS_API, params=params)\n",
    "    j = r.json()\n",
    "    for pid, pg in j.get(\"query\", {}).get(\"pages\", {}).items():\n",
    "        if \"imageinfo\" in pg:\n",
    "            return pg[\"imageinfo\"][0]\n",
    "    return None\n",
    "\n",
    "def extract_caption_from_extmetadata(extmetadata):\n",
    "    if not extmetadata:\n",
    "        return \"\"\n",
    "    for key in (\"ImageDescription\", \"ObjectName\", \"ImageDescriptionPlain\"):\n",
    "        if key in extmetadata:\n",
    "            v = extmetadata[key]\n",
    "            if isinstance(v, dict):\n",
    "                return v.get(\"value\",\"\").strip()\n",
    "            return str(v).strip()\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b900b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monuments:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  warn: skip image File:Allah-green.svg ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n",
      "  warn: skip image File:Commons-logo.svg HTTPSConnectionPool(host='commons.wikimedia.org', port=443): Read timed out. (read timeout=20)\n",
      "  warn: skip image File:Detail of plant motifs on Taj Mahal wall.jpg HTTPSConnectionPool(host='upload.wikimedia.org', port=443): Read timed out. (read timeout=40)\n",
      "  warn: skip image File:Taj Mahal - Mausoleum der Liebe (CC BY-SA 4.0).webm HTTPSConnectionPool(host='upload.wikimedia.org', port=443): Read timed out.\n",
      "  warn: skip image File:Taj Mahal 9794.jpg ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monuments:  33%|███▎      | 1/3 [05:28<10:56, 328.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Taj Mahal: text -> /Users/jaydobariya/Desktop/RAG Project/data/raw/text/Taj Mahal.txt, images -> 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monuments:  67%|██████▋   | 2/3 [06:36<02:55, 175.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Qutb Minar: text -> /Users/jaydobariya/Desktop/RAG Project/data/raw/text/Qutb Minar.txt, images -> 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monuments: 100%|██████████| 3/3 [09:08<00:00, 182.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Red Fort: text -> /Users/jaydobariya/Desktop/RAG Project/data/raw/text/Red Fort.txt, images -> 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/ingest_summary.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 75\u001b[39m\n\u001b[32m     72\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mError ingesting\u001b[39m\u001b[33m\"\u001b[39m, title, e)\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Write a summary file\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/ingest_summary.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mw\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     76\u001b[39m     json.dump(results, f, ensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m, indent=\u001b[32m2\u001b[39m)\n\u001b[32m     78\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDone. Summary saved to data/ingest_summary.json\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.14/lib/python/site-packages/IPython/core/interactiveshell.py:344\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    339\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    342\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/ingest_summary.json'"
     ]
    }
   ],
   "source": [
    "# Cell 7 — main ingest loop: save text, download up to max_images images per page, write metadata\n",
    "from pathlib import Path\n",
    "import json, os\n",
    "\n",
    "MAX_IMAGES_PER_PAGE = 40\n",
    "\n",
    "results = []\n",
    "\n",
    "for title in tqdm(MONUMENTS, desc=\"Monuments\"):\n",
    "    try:\n",
    "        canonical_title, text = fetch_wikipedia_text(title)\n",
    "        safe_name = sanitize_filename(canonical_title)\n",
    "        # Save raw text\n",
    "        txt_path = RAW_TEXT / (safe_name + \".txt\")\n",
    "        txt_path.write_text(text or \"\", encoding=\"utf8\")\n",
    "        # Find image files referenced on page\n",
    "        img_files = fetch_image_file_titles(canonical_title)\n",
    "        # Prepare image folder\n",
    "        img_folder = RAW_IMAGES / safe_name\n",
    "        img_folder.mkdir(parents=True, exist_ok=True)\n",
    "        meta_records = []\n",
    "        # Page metadata record\n",
    "        page_rec = {\"type\":\"page\", \"title\": canonical_title, \"text_file\": str(txt_path), \"word_count\": len((text or \"\").split())}\n",
    "        meta_records.append(page_rec)\n",
    "        downloaded = 0\n",
    "        for ft in img_files:\n",
    "            if downloaded >= MAX_IMAGES_PER_PAGE:\n",
    "                break\n",
    "            try:\n",
    "                info = get_commons_image_info(ft)\n",
    "                if not info or not info.get(\"url\"): \n",
    "                    continue\n",
    "                url = info.get(\"url\")\n",
    "                fname = ft.split(\":\",1)[-1]\n",
    "                fname = sanitize_filename(unquote(fname))\n",
    "                local_path = img_folder / fname\n",
    "                # download if missing\n",
    "                if not local_path.exists():\n",
    "                    resp = session.get(url, stream=True, timeout=40)\n",
    "                    time.sleep(SLEEP)\n",
    "                    resp.raise_for_status()\n",
    "                    with open(local_path, \"wb\") as f:\n",
    "                        for chunk in resp.iter_content(1024*8):\n",
    "                            if chunk:\n",
    "                                f.write(chunk)\n",
    "                caption = extract_caption_from_extmetadata(info.get(\"extmetadata\", {}))\n",
    "                rec = {\n",
    "                    \"type\":\"image\",\n",
    "                    \"monument_title\": canonical_title,\n",
    "                    \"file_title\": ft,\n",
    "                    \"image_url\": url,\n",
    "                    \"local_path\": str(local_path),\n",
    "                    \"caption\": caption,\n",
    "                    \"mime\": info.get(\"mime\"),\n",
    "                    \"width\": info.get(\"width\"),\n",
    "                    \"height\": info.get(\"height\"),\n",
    "                    \"extmetadata_keys\": list(info.get(\"extmetadata\", {}).keys())\n",
    "                }\n",
    "                meta_records.append(rec)\n",
    "                downloaded += 1\n",
    "            except Exception as e:\n",
    "                print(\"  warn: skip image\", ft, e)\n",
    "                continue\n",
    "        # write per-monument meta jsonl (page first then images)\n",
    "        meta_file = META / (safe_name + \".jsonl\")\n",
    "        with open(meta_file, \"w\", encoding=\"utf8\") as mf:\n",
    "            for r in meta_records:\n",
    "                mf.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "        results.append({\"title\": canonical_title, \"text_file\": str(txt_path), \"meta_file\": str(meta_file), \"images_downloaded\": downloaded})\n",
    "        print(f\"Saved {canonical_title}: text -> {txt_path}, images -> {downloaded}\")\n",
    "    except Exception as e:\n",
    "        print(\"Error ingesting\", title, e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "838add0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text files: 3\n",
      "Meta files: 3\n",
      "Image files total: 89\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 — summary / quick checks\n",
    "from pathlib import Path\n",
    "import glob\n",
    "print(\"Text files:\", len(list(RAW_TEXT.glob(\"*.txt\"))))\n",
    "print(\"Meta files:\", len(list(META.glob(\"*.jsonl\"))))\n",
    "img_count = sum(1 for _ in RAW_IMAGES.rglob(\"*.*\"))\n",
    "print(\"Image files total:\", img_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85c5dc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned: Qutb Minar.txt -> ../data/processed/Qutb Minar.txt\n",
      "Cleaned: Red Fort.txt -> ../data/processed/Red Fort.txt\n",
      "Cleaned: Taj Mahal.txt -> ../data/processed/Taj Mahal.txt\n",
      "Processed text saved to data/processed/\n"
     ]
    }
   ],
   "source": [
    "# Cell 9 — light text cleaning and save processed copy\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def clean_wikipedia_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = re.sub(r'\\[\\s*[0-9a-zA-Z.,:;–—#\\s-]{1,30}\\s*\\]', '', text)\n",
    "    text = re.sub(r'\\(citation needed\\)', '', text, flags=re.IGNORECASE)\n",
    "    # cut at common tail sections\n",
    "    for marker in [\"== References ==\", \"==References==\", \"== External links ==\", \"==External links==\", \"== See also ==\", \"==See also==\"]:\n",
    "        idx = text.find(marker)\n",
    "        if idx != -1:\n",
    "            text = text[:idx]\n",
    "    text = re.sub(r'=+', ' ', text)\n",
    "    text = text.replace(\"&nbsp;\", \" \").replace(\"&amp;\", \"&\")\n",
    "    text = re.sub(r'\\n\\s*\\n+', '\\n\\n', text)\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "PROCESSED = Path(\"../data/processed\")\n",
    "PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for txt in RAW_TEXT.glob(\"*.txt\"):\n",
    "    raw = txt.read_text(encoding=\"utf8\")\n",
    "    cleaned = clean_wikipedia_text(raw)\n",
    "    out = PROCESSED / txt.name  # same filename\n",
    "    out.write_text(cleaned, encoding=\"utf8\")\n",
    "    print(\"Cleaned:\", txt.name, \"->\", out)\n",
    "print(\"Processed text saved to data/processed/\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
