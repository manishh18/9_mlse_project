{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "924dc736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02-preprocess-chunk.ipynb\n",
    "# This file contains the notebook-equivalent cells for preprocessing and chunking\n",
    "# of cleaned Wikipedia text into retrieval-ready chunks.\n",
    "# Paste into a Jupyter cell or save as a .py and run sections sequentially.\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Cell 0 - Header / Notes\n",
    "# ----------------------------\n",
    "# Notebook: 02 - Preprocess & Chunk\n",
    "# Goal: clean processed Wikipedia text and chunk it into retrieval-sized\n",
    "# pieces. Output: data/processed/chunks.jsonl and data/processed/chunks_preview.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "023dc6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /Users/jaydobariya/Desktop/RAG Project\n",
      "RAW_TEXT_DIR: /Users/jaydobariya/Desktop/RAG Project/data/raw/text\n",
      "PROCESSED_DIR: /Users/jaydobariya/Desktop/RAG Project/data/processed\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Cell 1 - Setup folders & paths\n",
    "# ----------------------------\n",
    "from pathlib import Path\n",
    "import os\n",
    "ROOT = Path.cwd().parent\n",
    "RAW_TEXT_DIR = ROOT / \"data\" / \"raw\" / \"text\" # or use data/processed if you saved cleaned text there\n",
    "PROCESSED_DIR = ROOT / \"data\" / \"processed\"\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"RAW_TEXT_DIR:\", RAW_TEXT_DIR)\n",
    "print(\"PROCESSED_DIR:\", PROCESSED_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58cf572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Cell 2 - Cleaning & chunking helpers\n",
    "# ----------------------------\n",
    "import re, json\n",
    "\n",
    "def clean_wikipedia_text(text):\n",
    "    \"\"\"\n",
    "    Light cleaning for Wikipedia extracts: remove citation brackets,\n",
    "    remove citation-needed tags, cut off reference/external links sections,\n",
    "    collapse whitespace.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # remove bracketed citations like [1], [12], [a]\n",
    "    text = re.sub(r'\\[\\s*[0-9a-zA-Z.,:;–—#\\s-]{1,30}\\s*\\]', '', text)\n",
    "    text = re.sub(r'\\(citation needed\\)', '', text, flags=re.IGNORECASE)\n",
    "    # cut off common trailing sections\n",
    "    cut_markers = [\"== References ==\", \"==References==\", \"== External links ==\", \"==External links==\",\n",
    "                   \"== See also ==\", \"==See also==\", \"== Notes ==\", \"==Notes==\", \"== Further reading ==\"]\n",
    "    for marker in cut_markers:\n",
    "        idx = text.find(marker)\n",
    "        if idx != -1:\n",
    "            text = text[:idx]\n",
    "    text = re.sub(r'=+', ' ', text)  # remove leftover heading markers\n",
    "    text = text.replace(\"&nbsp;\", \" \").replace(\"&amp;\", \"&\")\n",
    "    text = re.sub(r'\\n\\s*\\n+', '\\n\\n', text)\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def chunk_text_by_word_count(text, max_words=220, min_words=60):\n",
    "    \"\"\"\n",
    "    Chunk text into pieces with up to max_words. Try to split at paragraph\n",
    "    boundaries when possible. Return list of chunk strings.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    paragraphs = [p.strip() for p in text.split('\\n') if p.strip()]\n",
    "    chunks = []\n",
    "    cur = []\n",
    "    cur_count = 0\n",
    "    for p in paragraphs:\n",
    "        words = p.split()\n",
    "        wcount = len(words)\n",
    "        if cur_count + wcount <= max_words:\n",
    "            cur.append(p)\n",
    "            cur_count += wcount\n",
    "        else:\n",
    "            if cur_count == 0 and wcount > max_words:\n",
    "                # split a long paragraph into windows\n",
    "                i = 0\n",
    "                while i < len(words):\n",
    "                    part = words[i:i+max_words]\n",
    "                    chunks.append(' '.join(part))\n",
    "                    i += max_words\n",
    "                cur = []\n",
    "                cur_count = 0\n",
    "            else:\n",
    "                if cur:\n",
    "                    chunk_text = '\\n\\n'.join(cur).strip()\n",
    "                    if len(chunk_text.split()) >= min_words:\n",
    "                        chunks.append(chunk_text)\n",
    "                    else:\n",
    "                        if chunks:\n",
    "                            chunks[-1] += '\\n\\n' + chunk_text\n",
    "                        else:\n",
    "                            chunks.append(chunk_text)\n",
    "                cur = [p]\n",
    "                cur_count = wcount\n",
    "    # finalize remainder\n",
    "    if cur:\n",
    "        chunk_text = '\\n\\n'.join(cur).strip()\n",
    "        if chunks and len(chunk_text.split()) < min_words:\n",
    "            chunks[-1] += '\\n\\n' + chunk_text\n",
    "        else:\n",
    "            chunks.append(chunk_text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "218b7dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded image metadata for 3 monuments\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Cell 3 - Build image map from metadata (optional)\n",
    "# ----------------------------\n",
    "meta_dir = ROOT / \"data\" / \"meta\"\n",
    "image_map = {}\n",
    "if meta_dir.exists():\n",
    "    for mf in meta_dir.glob(\"*.jsonl\"):\n",
    "        title = mf.stem\n",
    "        recs = []\n",
    "        with open(mf, encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                try:\n",
    "                    r = json.loads(line)\n",
    "                except Exception:\n",
    "                    continue\n",
    "                if r.get(\"type\") == \"image\":\n",
    "                    recs.append(r)\n",
    "        image_map[title] = recs\n",
    "print(\"Loaded image metadata for\", len(image_map), \"monuments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "956ab8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 69 chunks to /Users/jaydobariya/Desktop/RAG Project/data/processed/chunks.jsonl\n",
      "Preview saved to /Users/jaydobariya/Desktop/RAG Project/data/processed/chunks_preview.csv\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Cell 4 - Create chunks.jsonl and preview CSV\n",
    "# ----------------------------\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "RAW_TEXT_SRC = ROOT / \"data\" / \"raw\" / \"text\"\n",
    "OUT_JSONL = ROOT / \"data\" / \"processed\" / \"chunks.jsonl\"\n",
    "OUT_CSV = ROOT / \"data\" / \"processed\" / \"chunks_preview.csv\"\n",
    "\n",
    "max_words = 220\n",
    "min_words = 60\n",
    "\n",
    "count = 0\n",
    "preview = []\n",
    "OUT_JSONL.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(OUT_JSONL, \"w\", encoding=\"utf8\") as outf:\n",
    "    for txt in sorted(RAW_TEXT_SRC.glob(\"*.txt\")):\n",
    "        title = txt.stem\n",
    "        raw = txt.read_text(encoding=\"utf8\")\n",
    "        cleaned = clean_wikipedia_text(raw)\n",
    "        chunks = chunk_text_by_word_count(cleaned, max_words=max_words, min_words=min_words)\n",
    "        imgs = image_map.get(title, [])\n",
    "        img_refs = [{\"local_path\": r.get(\"local_path\"), \"caption\": r.get(\"caption\"), \"image_url\": r.get(\"image_url\")} for r in imgs]\n",
    "        for i, c in enumerate(chunks):\n",
    "            rec = {\n",
    "                \"id\": f\"{title}__{i}\",\n",
    "                \"title\": title,\n",
    "                \"text\": c,\n",
    "                \"word_count\": len(c.split()),\n",
    "                \"source_file\": str(txt),\n",
    "                \"images\": img_refs,\n",
    "                \"chunk_index\": i\n",
    "            }\n",
    "            outf.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "            count += 1\n",
    "            if len(preview) < 200:\n",
    "                preview.append({\"id\": rec[\"id\"], \"title\": title, \"word_count\": rec[\"word_count\"], \"snippet\": c[:300].replace(\"\\n\",\" \") + (\"...\" if len(c)>300 else \"\")})\n",
    "print(\"Wrote\", count, \"chunks to\", OUT_JSONL)\n",
    "\n",
    "with open(OUT_CSV, \"w\", newline=\"\", encoding=\"utf8\") as cf:\n",
    "    writer = csv.DictWriter(cf, fieldnames=[\"id\",\"title\",\"word_count\",\"snippet\"])\n",
    "    writer.writeheader()\n",
    "    for r in preview:\n",
    "        writer.writerow(r)\n",
    "print(\"Preview saved to\", OUT_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ada8b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 69\n",
      "Chunks per title: {'Qutb Minar': 14, 'Red Fort': 21, 'Taj Mahal': 34}\n",
      "Mean words: 164.53623188405797 Median: 162\n",
      "\n",
      "Example chunk id: Qutb Minar__0\n",
      "The Qutb Minar, also spelled Qutub Minar and Qutab Minar, is a minaret and victory tower, built during the Delhi sultanate, and comprising the Qutb complex, a UNESCO World Heritage Site in Mehrauli, South Delhi, India. It was mostly built between 1199 and 1220, contains 399 steps, and is one of the most-frequented heritage spots in the city. After defeating Prithviraj Chauhan, the last Hindu ruler of Delhi before the Ghurid conquest of the region, Qutab-ud-din Aibak initiated the construction of the victory tower, but only managed to finish the first level. It was to mark the beginning of Islamic rule in the region. Successive dynasties of the Delhi Sultanate continued the construction, and, in 1368, Firuz Shah Tughlaq rebuilt the top parts and added a cupola.\n",
      "\n",
      "It can be compared to the 62 ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Cell 5 - Sanity checks / stats\n",
    "# ----------------------------\n",
    "import statistics\n",
    "\n",
    "total = 0\n",
    "wcs = []\n",
    "titles = {}\n",
    "with open(OUT_JSONL, encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        total += 1\n",
    "        rec = json.loads(line)\n",
    "        wcs.append(rec.get(\"word_count\", 0))\n",
    "        titles[rec[\"title\"]] = titles.get(rec[\"title\"], 0) + 1\n",
    "\n",
    "print(\"Total chunks:\", total)\n",
    "print(\"Chunks per title:\", titles)\n",
    "if wcs:\n",
    "    print(\"Mean words:\", statistics.mean(wcs), \"Median:\", statistics.median(wcs))\n",
    "\n",
    "# show first chunk example\n",
    "with open(OUT_JSONL, encoding=\"utf8\") as f:\n",
    "    first = json.loads(f.readline())\n",
    "print('\\nExample chunk id:', first['id'])\n",
    "print(first['text'][:800], \"...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "443135e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote /Users/jaydobariya/Desktop/RAG Project/data/processed/chunks_with_image.jsonl ; linked 69 chunks to an image (simple overlap)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Cell 6 - (Optional) Link best image per chunk using caption overlap\n",
    "# ----------------------------\n",
    "import re\n",
    "\n",
    "def best_image_for_chunk(chunk_text, images):\n",
    "    if not images:\n",
    "        return None\n",
    "    best = None\n",
    "    best_score = -1\n",
    "    words = set(re.sub(r'\\W+', ' ', chunk_text.lower()).split())\n",
    "    for img in images:\n",
    "        cap = (img.get('caption') or '').lower()\n",
    "        cap_words = set(re.sub(r'\\W+', ' ', cap).split())\n",
    "        score = len(words & cap_words)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best = img\n",
    "    return best if best_score > 0 else None\n",
    "\n",
    "IN_F = OUT_JSONL\n",
    "OUT_F = ROOT / \"data\" / \"processed\" / \"chunks_with_image.jsonl\"\n",
    "linked = 0\n",
    "with open(IN_F, encoding=\"utf8\") as inf, open(OUT_F, \"w\", encoding=\"utf8\") as outf:\n",
    "    for line in inf:\n",
    "        rec = json.loads(line)\n",
    "        images = rec.get('images', [])\n",
    "        best = best_image_for_chunk(rec['text'], images)\n",
    "        rec['best_image'] = best\n",
    "        if best:\n",
    "            linked += 1\n",
    "        outf.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "print('Wrote', OUT_F, '; linked', linked, 'chunks to an image (simple overlap)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a7f21b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02-preprocess-chunk script complete.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Cell 7 - Next steps note\n",
    "# ----------------------------\n",
    "# After this run:\n",
    "# - you should have data/processed/chunks.jsonl and chunks_preview.csv\n",
    "# - next: run embeddings notebook to compute embeddings and build FAISS index\n",
    "# - the optional chunks_with_image.jsonl can help connect chunks to representative images\n",
    "\n",
    "print('02-preprocess-chunk script complete.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
