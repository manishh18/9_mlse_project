{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a2e481c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03-embeddings-index.ipynb\n",
    "# Purpose: compute embeddings for chunks.jsonl and build a FAISS index.\n",
    "# Outputs: embeddings/embeddings.npy, embeddings/faiss_index.bin, embeddings/metadata.jsonl\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Cell 0 - Header\n",
    "# ----------------------------\n",
    "# Notebook: 03 - Embeddings & FAISS Index\n",
    "# Use sentence-transformers to compute text embeddings and FAISS for retrieval index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b9ffe7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 69 chunks\n",
      "Example id: Qutb Minar__0 words: 213\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Cell 2 - Setup paths & load chunks\n",
    "# ----------------------------\n",
    "from pathlib import Path\n",
    "import json\n",
    "ROOT = Path.cwd().parent\n",
    "CHUNKS_FILE = ROOT / \"data\" / \"processed\" / \"chunks.jsonl\"\n",
    "EMB_DIR = ROOT / \"embeddings\"\n",
    "EMB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "chunks = []\n",
    "with open(CHUNKS_FILE, encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            chunks.append(json.loads(line))\n",
    "\n",
    "\n",
    "print(\"Loaded\", len(chunks), \"chunks\")\n",
    "if len(chunks) > 0:\n",
    "    print(\"Example id:\", chunks[0]['id'], \"words:\", chunks[0]['word_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "494bdd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Cell 3 - Choose embedding model\n",
    "# ----------------------------\n",
    "# For prototyping: all-MiniLM-L6-v2 is small & fast. For higher quality, use all-mpnet-base-v2.\n",
    "EMB_MODEL = \"all-MiniLM-L6-v2\"\n",
    "print(\"Embedding model:\", EMB_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ce19cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing embeddings for 69 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3/3 [00:00<00:00, 31.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (69, 384)\n",
      "Saved embeddings to /Users/jaydobariya/Desktop/RAG Project/embeddings/embeddings.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Cell 4 - Compute embeddings\n",
    "# ----------------------------\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = SentenceTransformer(EMB_MODEL)\n",
    "texts = [c['text'] for c in chunks]\n",
    "print(\"Computing embeddings for\", len(texts), \"texts...\")\n",
    "embeddings = model.encode(texts, show_progress_bar=True, convert_to_numpy=True)\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "\n",
    "# Save embeddings\n",
    "emb_path = EMB_DIR / \"embeddings.npy\"\n",
    "np.save(str(emb_path), embeddings)\n",
    "print(\"Saved embeddings to\", emb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4bc91ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 384\n",
      "Index size: 69\n",
      "Saved FAISS index to /Users/jaydobariya/Desktop/RAG Project/embeddings/faiss_index.bin\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Cell 5 - Build FAISS index\n",
    "# ----------------------------\n",
    "import faiss\n",
    "\n",
    "emb = np.load(str(emb_path))\n",
    "d = emb.shape[1]\n",
    "print(\"Embedding dimension:\", d)\n",
    "\n",
    "# Use IndexFlatL2 for simplicity (exact search)\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(emb.astype('float32'))\n",
    "print(\"Index size:\", index.ntotal)\n",
    "\n",
    "# Save index to disk\n",
    "faiss.write_index(index, str(EMB_DIR / \"faiss_index.bin\"))\n",
    "print(\"Saved FAISS index to\", EMB_DIR / \"faiss_index.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c1f580c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved metadata to /Users/jaydobariya/Desktop/RAG Project/embeddings/metadata.jsonl\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Cell 6 - Save metadata aligned with vectors\n",
    "# ----------------------------\n",
    "meta_file = EMB_DIR / \"metadata.jsonl\"\n",
    "with open(meta_file, \"w\", encoding=\"utf8\") as f:\n",
    "    for c in chunks:\n",
    "        m = {\"id\": c[\"id\"], \"title\": c[\"title\"], \"word_count\": c[\"word_count\"], \"source_file\": c.get(\"source_file\"), \"chunk_index\": c.get(\"chunk_index\")}\n",
    "        f.write(json.dumps(m, ensure_ascii=False) + \"\\n\")\n",
    "print(\"Saved metadata to\", meta_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87537d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test retrieve for 'white marble mausoleum in Agra':\n",
      "- Taj Mahal__0 score: 0.7491079568862915 title: Taj Mahal\n",
      " snippet: The Taj Mahal ( TAHJ mə-HAHL, TAHZH -⁠; Hindustani: [taːdʒ ˈmɛɦ(ɛ)l]; lit. 'Crown of the Palace') is an ivory-white marble mausoleum on the right bank of the river Yamuna in Agra, Uttar Pradesh, India ...\n",
      "\n",
      "- Taj Mahal__4 score: 0.7501313090324402 title: Taj Mahal\n",
      " snippet: The Taj Mahal incorporates and expands on design traditions of Indo-Islamic and Mughal architecture. Inspirations for the building came from Timurid and Mughal buildings including the Gur-e Amir in Sa ...\n",
      "\n",
      "- Taj Mahal__20 score: 0.7874093055725098 title: Taj Mahal\n",
      " snippet: The tomb complex was built mainly using brick and lime mortar. The external surface of the main tomb building and the interior of the main cenotaph chamber is veneered with white marble. The other int ...\n",
      "\n",
      "03-embeddings-index notebook complete. Files saved to embeddings/.\n",
      "Next: integrate retrieve() in rag_engine and test generation.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Cell 7 - Retrieval helper (define a function)\n",
    "# ----------------------------\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "encoder = SentenceTransformer(EMB_MODEL)\n",
    "index = faiss.read_index(str(EMB_DIR / \"faiss_index.bin\"))\n",
    "meta = [json.loads(l) for l in open(EMB_DIR / \"metadata.jsonl\", encoding=\"utf8\").read().splitlines()]\n",
    "chunks_by_id = {c['id']: c for c in chunks}\n",
    "\n",
    "def retrieve(query, k=4):\n",
    "    q_emb = encoder.encode([query], convert_to_numpy=True)\n",
    "    D, I = index.search(q_emb.astype('float32'), k)\n",
    "    results = []\n",
    "    for dist, idx in zip(D[0], I[0]):\n",
    "        if idx < 0:\n",
    "            continue\n",
    "        m = meta[idx]\n",
    "        chunk = chunks_by_id.get(m['id'], {})\n",
    "        results.append({\"id\": m['id'], \"title\": m['title'], \"distance\": float(dist), \"text\": chunk.get('text','')})\n",
    "    return results\n",
    "\n",
    "# Quick test\n",
    "print(\"Test retrieve for 'white marble mausoleum in Agra':\")\n",
    "res = retrieve(\"white marble mausoleum in Agra\", k=3)\n",
    "for r in res:\n",
    "    print(\"-\", r['id'], \"score:\", r['distance'], \"title:\", r['title'])\n",
    "    print(\" snippet:\", r['text'][:200].replace('\\n',' '), \"...\\n\")\n",
    "\n",
    "# ----------------------------\n",
    "# Cell 8 - Next steps note\n",
    "# ----------------------------\n",
    "print(\"03-embeddings-index notebook complete. Files saved to embeddings/.\")\n",
    "print(\"Next: integrate retrieve() in rag_engine and test generation.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
